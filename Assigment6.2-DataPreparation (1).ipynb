{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52055540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         movie_title  review_rating  \\\n",
      "0        Blue Beetle            7.0   \n",
      "1        Blue Beetle            6.0   \n",
      "2        Blue Beetle            7.0   \n",
      "3        Blue Beetle            6.0   \n",
      "4  Meg 2: The Trench            5.0   \n",
      "\n",
      "                                      review_content sentiment  \n",
      "0  MORE SPOILER-FREE MINI-REVIEWS @ https://www.m...      good  \n",
      "1  Maybe this should just have been called the \"B...      good  \n",
      "2         Blue beetle is very awesome!! WoW!ðŸ‘ðŸ»ðŸ‘ðŸ»ðŸ‘ðŸ»ðŸ‘ðŸ»      good  \n",
      "3  The Good: Light-hearted and family centric. Lo...      good  \n",
      "4  Now the \"Meg\" (2018) itself could never be cal...      good  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the URL\n",
    "url = \"https://raw.githubusercontent.com/jvo024/ads509-movie-scrape/main/datasets/all_tmbd_rt_data.csv\"\n",
    "\n",
    "# Read data from the URL into a DataFrame\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Display the first few rows of the DataFrame (optional)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f083e450",
   "metadata": {},
   "outputs": [],
   "source": [
    "def descriptive_stats(tokens, num_tokens = 5, verbose=True) :\n",
    "    \"\"\"\n",
    "        Given a list of tokens, print number of tokens, number of unique tokens,\n",
    "        number of characters, lexical diversity, and num_tokens most common\n",
    "        tokens. Return a list of\n",
    "    \"\"\"\n",
    "\n",
    "    # print number of tokens\n",
    "    print(\"Number of Tokens: \", len(tokens))\n",
    "\n",
    "    # print number of unique tokens\n",
    "    tokens_set = set(tokens)\n",
    "    print(\"Number of Unique Items in List: \", len(tokens_set))\n",
    "\n",
    "    # print number of characters\n",
    "    print(\"Total Number of Characters:\", len(str(tokens)))\n",
    "\n",
    "    # print lexical diversity\n",
    "    print(\"Lexical Diversity:\", len(set(tokens)) / float(len(tokens)))\n",
    "\n",
    "    # print num_tokens most common tokens\n",
    "    print(\"10 Most Common Tokens:\", Counter(chain(*tokens)).most_common(10))\n",
    "\n",
    "\n",
    "    return(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a955c8ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in c:\\users\\connie\\anaconda3\\lib\\site-packages (2.8.0)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\connie\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\connie\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\connie\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\connie\\anaconda3\\lib\\site-packages)\n",
      "DEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d79737f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Connie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Some punctuation variations\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import emoji\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "\n",
    "punctuation = set(punctuation) # speeds up comparison\n",
    "tw_punct = punctuation - {\"#\"}\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Stopwords\n",
    "sw = stopwords.words(\"english\")\n",
    "\n",
    "# Two useful regex\n",
    "whitespace_pattern = re.compile(r\"\\s+\")\n",
    "hashtag_pattern = re.compile(r\"^#[0-9a-zA-Z]+\")\n",
    "\n",
    "# It's handy to have a full set of emojis\n",
    "all_language_emojis = set()\n",
    "\n",
    "for country in emoji.EMOJI_DATA :\n",
    "    for em in emoji.EMOJI_DATA[country] :\n",
    "        all_language_emojis.add(em)\n",
    "\n",
    "# and now our functions\n",
    "def descriptive_stats(tokens, num_tokens = 5, verbose=True) :\n",
    "    \"\"\"\n",
    "        Given a list of tokens, print number of tokens, number of unique tokens,\n",
    "        number of characters, lexical diversity, and num_tokens most common\n",
    "        tokens. Return a list of\n",
    "    \"\"\"\n",
    "\n",
    "    # print number of tokens\n",
    "    print(\"Number of Tokens: \", len(tokens))\n",
    "\n",
    "    # print number of unique tokens\n",
    "    tokens_set = set(tokens)\n",
    "    print(\"Number of Unique Items in List: \", len(tokens_set))\n",
    "\n",
    "    # print number of characters\n",
    "    print(\"Total Number of Characters:\", len(str(tokens)))\n",
    "\n",
    "    # print lexical diversity\n",
    "    print(\"Lexical Diversity:\", len(set(tokens)) / float(len(tokens)))\n",
    "\n",
    "    # print num_tokens most common tokens\n",
    "    print(\"10 Most Common Tokens:\", Counter(chain(*tokens)).most_common(10))\n",
    "\n",
    "\n",
    "    return(0)\n",
    "\n",
    "\n",
    "\n",
    "def contains_emoji(s):\n",
    "\n",
    "    s = str(s)\n",
    "    emojis = [ch for ch in s if emoji.is_emoji(ch)]\n",
    "\n",
    "    return(len(emojis) > 0)\n",
    "\n",
    "\n",
    "def remove_stop(tokens) :\n",
    "    # modify this function to remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in tokens if word not in stop_words]\n",
    "    return (tokens)\n",
    "\n",
    "def remove_punctuation(text, punct_set=tw_punct) :\n",
    "    return(\"\".join([ch for ch in text if ch not in punct_set]))\n",
    "\n",
    "def tokenize(text) :\n",
    "    \"\"\" Splitting on whitespace rather than the book's tokenize function. That\n",
    "        function will drop tokens like '#hashtag' or '2A', which we need for Twitter. \"\"\"\n",
    "\n",
    "    # modify this function to return tokens\n",
    "    tokens = text.split()\n",
    "    return(text)\n",
    "\n",
    "def prepare(text, pipeline) :\n",
    "    tokens = str(text)\n",
    "\n",
    "    for transform in pipeline :\n",
    "        tokens = transform(tokens)\n",
    "\n",
    "    return(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "988512c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tokens:  988\n",
      "Number of Unique Items in List:  63\n",
      "Total Number of Characters: 988\n",
      "Lexical Diversity: 0.06376518218623482\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'chain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7700/394652978.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mreview_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'review_content'\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# Get the text from the 'review_content' column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreview_text\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Tokenize the text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mdescriptive_stats\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Pass the tokens to the descriptive_stats function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7700/2842805826.py\u001b[0m in \u001b[0;36mdescriptive_stats\u001b[1;34m(tokens, num_tokens, verbose)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[1;31m# print num_tokens most common tokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"10 Most Common Tokens:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'chain' is not defined"
     ]
    }
   ],
   "source": [
    "for index, row in df.iterrows():\n",
    "    review_text = row['review_content']  # Get the text from the 'review_content' column\n",
    "    tokens = tokenize(review_text)  # Tokenize the text\n",
    "    descriptive_stats(tokens)  # Pass the tokens to the descriptive_stats function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba89d2a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
