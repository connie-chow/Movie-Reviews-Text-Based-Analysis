{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "340d1813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         movie_title  review_rating  \\\n",
      "0        Blue Beetle            7.0   \n",
      "1        Blue Beetle            6.0   \n",
      "2        Blue Beetle            7.0   \n",
      "3        Blue Beetle            6.0   \n",
      "4  Meg 2: The Trench            5.0   \n",
      "\n",
      "                                      review_content sentiment  \n",
      "0  MORE SPOILER-FREE MINI-REVIEWS @ https://www.m...      good  \n",
      "1  Maybe this should just have been called the \"B...      good  \n",
      "2         Blue beetle is very awesome!! WoW!👏🏻👏🏻👍🏻👍🏻      good  \n",
      "3  The Good: Light-hearted and family centric. Lo...      good  \n",
      "4  Now the \"Meg\" (2018) itself could never be cal...      good  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the URL\n",
    "url = \"https://raw.githubusercontent.com/jvo024/ads509-movie-scrape/main/datasets/all_tmbd_rt_data.csv\"\n",
    "\n",
    "# Read data from the URL into a DataFrame\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Display the first few rows of the DataFrame (optional)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedb5f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def descriptive_stats(tokens, num_tokens = 5, verbose=True) :\n",
    "    \"\"\"\n",
    "        Given a list of tokens, print number of tokens, number of unique tokens,\n",
    "        number of characters, lexical diversity, and num_tokens most common\n",
    "        tokens. Return a list of\n",
    "    \"\"\"\n",
    "\n",
    "    # print number of tokens\n",
    "    print(\"Number of Tokens: \", len(tokens))\n",
    "\n",
    "    # print number of unique tokens\n",
    "    tokens_set = set(tokens)\n",
    "    print(\"Number of Unique Items in List: \", len(tokens_set))\n",
    "\n",
    "    # print number of characters\n",
    "    print(\"Total Number of Characters:\", len(str(tokens)))\n",
    "\n",
    "    # print lexical diversity\n",
    "    print(\"Lexical Diversity:\", len(set(tokens)) / float(len(tokens)))\n",
    "\n",
    "    # print num_tokens most common tokens\n",
    "    print(\"10 Most Common Tokens:\", Counter(chain(*tokens)).most_common(10))\n",
    "\n",
    "\n",
    "    return(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccea299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some punctuation variations\n",
    "punctuation = set(punctuation) # speeds up comparison\n",
    "tw_punct = punctuation - {\"#\"}\n",
    "\n",
    "# Stopwords\n",
    "sw = stopwords.words(\"english\")\n",
    "\n",
    "# Two useful regex\n",
    "whitespace_pattern = re.compile(r\"\\s+\")\n",
    "hashtag_pattern = re.compile(r\"^#[0-9a-zA-Z]+\")\n",
    "\n",
    "# It's handy to have a full set of emojis\n",
    "all_language_emojis = set()\n",
    "\n",
    "for country in emoji.EMOJI_DATA :\n",
    "    for em in emoji.EMOJI_DATA[country] :\n",
    "        all_language_emojis.add(em)\n",
    "\n",
    "# and now our functions\n",
    "def descriptive_stats(tokens, num_tokens = 5, verbose=True) :\n",
    "    \"\"\"\n",
    "        Given a list of tokens, print number of tokens, number of unique tokens,\n",
    "        number of characters, lexical diversity, and num_tokens most common\n",
    "        tokens. Return a list of\n",
    "    \"\"\"\n",
    "\n",
    "    # print number of tokens\n",
    "    print(\"Number of Tokens: \", len(tokens))\n",
    "\n",
    "    # print number of unique tokens\n",
    "    tokens_set = set(tokens)\n",
    "    print(\"Number of Unique Items in List: \", len(tokens_set))\n",
    "\n",
    "    # print number of characters\n",
    "    print(\"Total Number of Characters:\", len(str(tokens)))\n",
    "\n",
    "    # print lexical diversity\n",
    "    print(\"Lexical Diversity:\", len(set(tokens)) / float(len(tokens)))\n",
    "\n",
    "    # print num_tokens most common tokens\n",
    "    print(\"10 Most Common Tokens:\", Counter(chain(*tokens)).most_common(10))\n",
    "\n",
    "\n",
    "    return(0)\n",
    "\n",
    "\n",
    "\n",
    "def contains_emoji(s):\n",
    "\n",
    "    s = str(s)\n",
    "    emojis = [ch for ch in s if emoji.is_emoji(ch)]\n",
    "\n",
    "    return(len(emojis) > 0)\n",
    "\n",
    "\n",
    "def remove_stop(tokens) :\n",
    "    # modify this function to remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in tokens if word not in stop_words]\n",
    "    return (tokens)\n",
    "\n",
    "def remove_punctuation(text, punct_set=tw_punct) :\n",
    "    return(\"\".join([ch for ch in text if ch not in punct_set]))\n",
    "\n",
    "def tokenize(text) :\n",
    "    \"\"\" Splitting on whitespace rather than the book's tokenize function. That\n",
    "        function will drop tokens like '#hashtag' or '2A', which we need for Twitter. \"\"\"\n",
    "\n",
    "    # modify this function to return tokens\n",
    "    return(text)\n",
    "\n",
    "def prepare(text, pipeline) :\n",
    "    tokens = str(text)\n",
    "\n",
    "    for transform in pipeline :\n",
    "        tokens = transform(tokens)\n",
    "\n",
    "    return(tokens)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
